{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cross_validation import KFold\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.stats import itemfreq\n",
    "from sklearn import neighbors, linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.set_printoptions(precision=3, linewidth=100)\n",
    "\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "os.chdir('/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_img = cv2.imread('/Users/amikar/Downloads/600u6z4invkz.jpg',cv2.IMREAD_COLOR)\n",
    "#cv2.imshow('image',load_img)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntropy(signal):\n",
    "    lensig=signal.size\n",
    "    symset=list(set(signal))\n",
    "    numsym=len(symset)\n",
    "    probabability_distribution=[np.size(signal[signal==i])/(1.0*lensig) for i in symset]\n",
    "    entropy=np.sum([p*np.log2(1.0/p) for p in probabability_distribution])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateEntropyNeighbourhood(artwork, neighbourhood):\n",
    "    image = cv2.imread(artwork)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    colorIm=np.array(image)\n",
    "    grayIm=np.array(gray_image)\n",
    "    \n",
    "    N=neighbourhood\n",
    "    S=grayIm.shape\n",
    "    E=np.array(grayIm)\n",
    "    \n",
    "    for row in range(S[0]): \n",
    "            for col in range (S[1]): \n",
    "                    Lx=np.max([0,col-N]) \n",
    "                    Ux=np.min([S[1],col+N])\n",
    "                    Ly=np.max([0,row-N])\n",
    "                    Uy=np.min([S[0],row+N])\n",
    "                    # makes region 1-D\n",
    "                    region=grayIm[Ly:Uy,Lx:Ux].flatten()\n",
    "                    E[row,col]=getEntropy(region)\n",
    "    \n",
    "    average=np.mean(E)\n",
    "    return average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDTM(artwork, neighbourhood):\n",
    "    image = cv2.imread(artwork)\n",
    "    image32f = np.float32(image)\n",
    "    mu    = cv2.blur(image32f,(neighbourhood,neighbourhood))\n",
    "    mu2   = cv2.blur(cv2.multiply(image32f,image32f), (neighbourhood,neighbourhood))\n",
    "    sigma = cv2.sqrt( mu2 - cv2.multiply(mu, mu) )\n",
    "    return np.mean(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5454607"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDTM('/Users/amikar/Downloads/new2.jpg',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_info(test_info, dir):\n",
    "\tif dir == 'test':\n",
    "\t\timages = list(set(list(test_info.image1.unique()) + list(test_info.image2.unique())))\n",
    "\t\tinfo = pd.DataFrame(np.array(images).reshape((-1, 1)), columns = ['filename'])\n",
    "\t\t#print info\n",
    "\telse:\n",
    "\t\tinfo = test_info\n",
    "\t\n",
    "\tinfo['pixelsx'] = np.nan\n",
    "\tinfo['pixelsy'] = np.nan\n",
    "\tinfo['size_bytes'] = np.nan\n",
    "\t#info['entropy'] = np.nan\n",
    "\tinfo['entropy1'] = np.nan\n",
    "\tinfo['entropy5'] = np.nan\n",
    "\tinfo['entropy10'] = np.nan\n",
    "\tinfo['entropy15'] = np.nan\n",
    "\tinfo['dtm1'] = np.nan\n",
    "\tinfo['dtm5'] = np.nan\n",
    "\tinfo['dtm10'] = np.nan\n",
    "\tinfo['dtm15'] = np.nan\n",
    "# \tinfo['entropy20'] = np.nan\n",
    "#\tinfo['dtm'] = np.nan\n",
    "\n",
    "\t\n",
    "\tj = 0\n",
    "\tfor i in info.index.values:\n",
    "\t\tj += 1        \n",
    "\t\ttry:\n",
    "\t\t\t#print i\n",
    "\t\t\t#fil = 'C:\\\\Users\\\\swaroop\\\\Downloads\\\\painters\\\\ImageSets\\\\Wyeths\\\\'+dir+'\\\\'+info.loc[i, 'filename']\n",
    "\t\t\tfil = '/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/'+info.loc[i, 'filename']\n",
    "\t\t\tprint fil\n",
    "\t\t\tim = Image.open('/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/'+info.loc[i, 'filename'])\n",
    "\t\t\t#print im\n",
    "\t\t\t#print im.size\n",
    "\t\t\tinfo.loc[i, 'pixelsx'], info.loc[i, 'pixelsy'] = im.size\n",
    "\t\t\t#im = cv2.imread(dir+'/'+info.loc[i, 'new_filename'])\n",
    "\t\t\t#info.loc[i, 'pixelsx'], info.loc[i, 'pixelsy'] = im.shape[0:2]\n",
    "\t\t\tinfo.loc[i, 'size_bytes'] = os.path.getsize(info.loc[i, 'filename'])\n",
    "\t\t\t#info.loc[i, 'entropy'] = calculateEntropyNeighbourhood(fil, 1)\n",
    "\t\t\t#print calculateEntropyNeighbourhood(fil, 1)\n",
    "#\t\t\tinfo.loc[i, 'dtm'] = getDTM(fil)\n",
    "\t\t\tinfo.loc[i, 'entropy1'] = calculateEntropyNeighbourhood(fil, 1)\n",
    "\t\t\tinfo.loc[i, 'entropy5'] = calculateEntropyNeighbourhood(fil, 5)\n",
    "\t\t\tinfo.loc[i, 'entropy10'] = calculateEntropyNeighbourhood(fil, 10)\n",
    "\t\t\tinfo.loc[i, 'entropy15'] = calculateEntropyNeighbourhood(fil, 15)\n",
    "\t\t\tinfo.loc[i, 'dtm1'] = getDTM(fil, 1)\n",
    "\t\t\tinfo.loc[i, 'dtm5'] = getDTM(fil, 5)\n",
    "\t\t\tinfo.loc[i, 'dtm10'] = getDTM(fil, 10)\n",
    "\t\t\tinfo.loc[i, 'dtm15'] = getDTM(fil, 15)\n",
    "# \t\t\tinfo.loc[i, 'entropy20'] = calculateEntropyNeighbourhood(fil, 20)\n",
    "\t\texcept:\n",
    "\t\t\tprint dir+'\\\\'+info.loc[i, 'filename']\n",
    "\t\tif (j%10 == 0):\n",
    "\t\t\tprint '',\n",
    "\tinfo=info.dropna()\n",
    "\tprint 'info shape',info.shape\n",
    "\treturn info.rename(columns={'filename' : 'new_filename'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t = make_pairs(train_info)\t\n",
    "def make_pairs(train_info):\n",
    "\tartists = train_info.artist.unique()\n",
    "\n",
    "\tn = train_info.groupby('artist').size()\n",
    "\tn = (2*n**2).sum() \n",
    "\tt = pd.DataFrame(np.zeros((n, 4)), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\ti = 0\n",
    "\tj = 0\n",
    "\tfor m in artists:\n",
    "\t\t\n",
    "\t\ta = train_info[train_info.artist==m][['artist', 'new_filename']].values\n",
    "\t\tuse = train_info[train_info.artist != m].index.values\n",
    "\t\tnp.random.shuffle(use)\n",
    "\t\t#print a.shape, use.shape\n",
    "\t\tnm = np.mean([a.shape[0]**2, train_info[train_info.artist != m].shape[0] ])\n",
    "\n",
    "\t\tuse = use[0:nm]\n",
    "\t\tb = train_info[train_info.artist!=m][['artist', 'new_filename']].ix[use, :].values\n",
    "\t\t#print nm, use.shape, b.shape\n",
    "\t\ta2 = pd.DataFrame(np.concatenate([np.repeat(a[:, 0], a.shape[0]).reshape((-1,1)), np.repeat(a[:, 1], a.shape[0]).reshape((-1,1)), np.tile(a, (a.shape[0], 1))], axis=1), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\t\ta2 = a2.loc[0:nm, :]\n",
    "\t\tb2 = pd.DataFrame(np.concatenate([np.tile(a, (a.shape[0], 1))[0:b.shape[0], :], b], axis=1), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "\t\t#print j, i, a2.shape[0], b2.shape[0]\n",
    "\t\t#print b2\n",
    "\t\tt.iloc[i:i+a2.shape[0], :] = a2.values\n",
    "\t\tt.iloc[i+a2.shape[0]:i+a2.shape[0]+b2.shape[0], :] = b2.values\n",
    "\t\ti += a2.shape[0] +b2.shape[0]\n",
    "\t\tj += 1\n",
    "\t\n",
    "\tt = t[~t.image2.isin([np.nan, 0])]\t\n",
    "\t#return t[t.image1 > t.image2]\t\n",
    "\treturn t.drop_duplicates(subset=['artist1', 'artist2','image1', 'image2'], keep=False)\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, y_train, x_cv, y_cv = prep_data([train_info, None], 'cv')\t\n",
    "#x_test, y_test = prep_data([None, submission_info], 'test')\t\n",
    "def prep_data(input, split):\n",
    "\tinfo = input[0]\n",
    "\tdata = input[1]\n",
    "\t\n",
    "\tif split=='cv':\n",
    "\t\t#artists = info.artist.unique()\n",
    "\t\tartists = info.artist\n",
    "\t\t#print artists\n",
    "\t\t#print 'hi', artists\n",
    "\t\tnp.random.shuffle(artists)\n",
    "\t\t\n",
    "\t\tinfo = get_image_info(info, 'train')\n",
    "\t\tinfo['bytes_per_pixel'] = 1.0*info['size_bytes']/(info['pixelsx']*info['pixelsy'])\n",
    "\t\tinfo['aspect_ratio'] = 1.0*info['pixelsx']/info['pixelsy']\t\n",
    "\t\t#train_artists = artists[0:int(0.8*len(artists))]\n",
    "\t\t#test_artists = artists[int(0.8*len(artists)):]\n",
    "\t\t#print artists\n",
    "\t\tprint 'hi',info[info.artist.isin(artists)].shape\n",
    "\t\t#train = make_pairs(info[info.artist.isin(artists)])\n",
    "\t\t#test = make_pairs(info[info.artist.isin(test_artists)])\n",
    "\t\t#print train.shape\n",
    "\t\t#train['in_train'] = True\n",
    "\t\t#test['in_train'] = True\n",
    "\t\tprint info.columns\n",
    "\t\tinfo['artist'] = info['artist'].map({'hudsonriver': 1, 'impressionist': 0})\n",
    "\t\ty_train = info['artist']\n",
    "\t\tx_train = info.drop(['artist', 'new_filename'], axis=1) \n",
    "\t\tprint x_train.columns\n",
    "\t\tprint y_train\n",
    "\t\tprint x_train\n",
    "\t\t#data['sameArtist'] = data['artist1'] == data['artist2']\n",
    "\t\t\n",
    "\tif split=='test':\n",
    "\n",
    "\t\tinfo = get_image_info(data, 'test')\n",
    "\t\tinfo['bytes_per_pixel'] = 1.0*info['size_bytes']/(info['pixelsx']*info['pixelsy'])\n",
    "\t\tinfo['aspect_ratio'] = 1.0*info['pixelsx']/info['pixelsy']\t\n",
    "\t\t\n",
    "\t\tdata['in_train'] = False\n",
    "\t\n",
    "\t\tif 'artist1' in data.columns:\n",
    "\t\t\tdata['sameArtist'] = data['artist1'] == data['artist2']\n",
    "\n",
    "\t\n",
    "\t#data2 = pd.merge(data, info[['new_filename', 'pixelsx', 'pixelsy', 'size_bytes', 'bytes_per_pixel', 'aspect_ratio']], how='left', left_on='image1', right_on='new_filename')\n",
    "\t#data2.drop('new_filename', 1, inplace=True)\n",
    "\t\n",
    "\t#data2 = pd.merge(data2, info[['new_filename', 'pixelsx', 'pixelsy', 'size_bytes', 'bytes_per_pixel', 'aspect_ratio']], how='left', left_on='image2', right_on='new_filename')\n",
    "\t#data2.drop('new_filename', 1, inplace=True)\n",
    "\t\n",
    "\t#x_train = data2[data2.in_train==True][['pixelsx_x', 'pixelsy_x', 'size_bytes_x', 'bytes_per_pixel_x', 'aspect_ratio_x', 'pixelsx_y', 'pixelsy_y', 'size_bytes_y', 'bytes_per_pixel_y', 'aspect_ratio_y']].values\n",
    "\t#x_test = data2[data2.in_train==False][['pixelsx_x', 'pixelsy_x', 'size_bytes_x', 'bytes_per_pixel_x', 'aspect_ratio_x', 'pixelsx_y', 'pixelsy_y', 'size_bytes_y', 'bytes_per_pixel_y', 'aspect_ratio_y']].values\n",
    "\t\n",
    "\t\n",
    "# \tif 'artist1' in data.columns: \n",
    "# \t\ty_train = data2[data2.in_train==True]['sameArtist'].values\n",
    "# \t\ty_test = data2[data2.in_train==False]['sameArtist'].values\n",
    "# \telse:\n",
    "# \t\ty_test = None\t\n",
    "\t\n",
    "\tif split=='cv':\t\t\n",
    "\t\treturn x_train, y_train, x_train, y_train  \n",
    " \tif split=='test':\n",
    "\t\treturn x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepping training and cv data\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Bierstadt-1863.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Bierstadt-1865-7.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Bierstadt-1869.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Bierstadt-1876-7.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Bierstadt-1895.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Church-1849.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Church-1851.jpeg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Church-1855.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Church-1857.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Church-1859.jpg\n",
      " /Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Cole-1827a.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Cole-1827b.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Cole-1836.jpg\n",
      "train\\Cole-1836.jpg\n",
      "/Users/amikar/Downloads/ImageSets/Pilot/Artwork_for_Testing/Cole-1844.jpg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "train_info = pd.read_csv('/Users/amikar/Downloads/ImageSets/pilot_medium.csv')\n",
    "#submission_info = pd.read_csv('submission_info.csv')\n",
    "print 'prepping training and cv data'\n",
    "x_train, y_train, x_cv, y_cv = prep_data([train_info, None], 'cv')\n",
    "\n",
    "print x_train.shape\n",
    "#np.savetxt('x_train_pilot.txt', x_train, fmt = '%1.3f' )\n",
    "#np.savetxt('y_train_pilot.txt', y_train, fmt = '%1.3f' )\n",
    "#x_train.to_csv(\"pd_x_train_pilot.txt\")\n",
    "#y_train.to_csv(\"pd_y_train_pilot.txt\")\n",
    "\n",
    "print (time.time() - start_time)/60 , \"minutes\"\n",
    "\n",
    "#print 'prepping test data'\n",
    "#x_test, y_test = prep_data([None, submission_info], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
